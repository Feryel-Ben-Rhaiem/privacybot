{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages from requirements.txt\n",
    "req_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/requirements.txt'\n",
    "with open(req_path, 'r') as file:\n",
    "    requirements = file.read().splitlines()\n",
    "\n",
    "for package in requirements:\n",
    "    !pip install {package}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragpipelines import *\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "import yaml\n",
    "config_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/config.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "!pip install trulens_eval==0.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trulens_eval.feedback import GroundTruthAgreement\n",
    "from trulens_eval import Tru, OpenAI, Feedback, TruLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = config['openai_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TruLens and OpenAI provider\n",
    "tru = Tru()\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation questions and answers\n",
    "eval_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/eval_questions.xlsx'\n",
    "qa_df = pd.read_excel(eval_path, sheet_name=\"Sheet1\")\n",
    "qa_set = [{\"query\": item[\"Question\"], \"response\": item[\"Answer\"]} for index, item in qa_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metrics\n",
    "# answer relevance\n",
    "f_qa_relevance = Feedback(\n",
    "    openai_provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context relevance\n",
    "f_qs_relevance = Feedback(\n",
    "    openai_provider.relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    ").on_input().on(TruLlama.select_source_nodes().node.text).aggregate(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groundedness (sometimes called faithfulness)\n",
    "f_groundedness = (\n",
    "    Feedback(openai_provider.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(TruLlama.select_source_nodes().node.text)\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth agreement (sometimes called answer correctness)\n",
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(qa_set).agreement_measure, name = \"Answer Correctness\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [f_qa_relevance, f_qs_relevance, f_groundedness, f_groundtruth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create TruLens recorder\n",
    "def get_trulens_recorder(query_engine, app_id):\n",
    "    return TruLlama(query_engine, feedbacks=metrics, app_id=app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for RAG pipelines evaluation\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, pipeline_class, app_id, api_key, document_path, llm_model, embed_model):\n",
    "        self.pipeline = pipeline_class(api_key, document_path, llm_model, embed_model)\n",
    "        self.query_engine = self.pipeline.query_engine\n",
    "        self.recorder = get_trulens_recorder(self.query_engine, app_id=app_id)\n",
    "\n",
    "    def evaluate(self, qa_set):\n",
    "        with self.recorder as recording:\n",
    "            for q in qa_set:\n",
    "                self.query_engine.query(q[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = config['openai_api_key']\n",
    "document_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/privacy-pmi.pdf'\n",
    "llm_model = config['llm_model']\n",
    "embed_model = config['embed_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates a given RAG pipeline by using the TruLens framework\n",
    "def evaluate_pipeline(pipeline, qa_set, app_id, tru, metrics):\n",
    "    # Initialize TruLens recorder for the pipeline\n",
    "    recorder = TruLlama(\n",
    "        app=pipeline.query_engine,  # Assuming the query_engine is the app here\n",
    "        feedbacks=metrics,\n",
    "        app_id=app_id\n",
    "    )\n",
    "\n",
    "    with recorder as recording:\n",
    "        for q in qa_set:\n",
    "            pipeline.query(q[\"query\"])\n",
    "\n",
    "    # Retrieve and return the records and feedback\n",
    "    records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "    return records, feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and evaluate each pipeline\n",
    "\n",
    "basic_pipeline = BasicRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                  llm_model=config['llm_model'], embed_model=config['embed_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(basic_pipeline, qa_set, \"0. Basic Query Engine\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_rag_without_rerank = SentenceWindowRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                                               llm_model=config['llm_model'], embed_model=config['embed_model'],\n",
    "                                                               rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(sentence_window_rag_without_rerank, qa_set, \"1. Sentence Window Query Engine - W/o Reranking\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_rag_with_rerank = SentenceWindowRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                                            llm_model=config['llm_model'], embed_model=config['embed_model'],\n",
    "                                                            rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(sentence_window_rag_with_rerank, qa_set, \"2. Sentence Window Query Engine - w/ re-ranking\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automerging_pipeline = AutoMergingRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                              llm_model=config['llm_model'], embed_model=config['embed_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(automerging_pipeline, qa_set, \"3. Automerging Query Engine - w/ re-ranking\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automerging_pipeline_with_prompting = AutoMergingRAGPipelineWithPrompting(\n",
    "    api_key=config['openai_api_key'],\n",
    "    document_path=document_path,\n",
    "    llm_model=config['llm_model'],\n",
    "    embed_model=config['embed_model']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(\n",
    "    automerging_pipeline_with_prompting,\n",
    "    qa_set,\n",
    "    \"4. Auto-Merging Query Engine - w/ re-ranking and prompt engineering\",\n",
    "    tru,\n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tru.stop_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# RADAR CHART\n",
    "\n",
    "# Data for the pipelines\n",
    "pipelines = [\n",
    "    \"Basic Query Engine\",\n",
    "    \"Sentence Window Query Engine - W/o Reranking\",\n",
    "    \"Sentence Window Query Engine - w/ Re-ranking\",\n",
    "    \"Auto-merging Query Engine - w/ Re-ranking\",\n",
    "    \"Auto-merging Query Engine - w/ Re-ranking and Prompt Engineering\"\n",
    "]\n",
    "\n",
    "# Metrics\n",
    "categories = ['Answer Correctness', 'Context Relevance', 'Groundedness', 'Answer Relevance']\n",
    "values = [\n",
    "    [0.78, 0.54, 0.51, 0.89],\n",
    "    [0.83, 0.46, 0.28, 0.83],\n",
    "    [0.43, 0.31, 0.55, 0.66],\n",
    "    [0.98, 0.56, 0.48, 0.81],\n",
    "    [0.98, 0.57, 0.40, 0.82]\n",
    "]\n",
    "\n",
    "# Radar chart setup\n",
    "N = len(categories)\n",
    "angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "values = [v + [v[0]] for v in values]  \n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
    "\n",
    "for i, pipeline in enumerate(pipelines):\n",
    "    ax.fill(angles, values[i], color=colors[i], alpha=0.3)\n",
    "    ax.plot(angles, values[i], color=colors[i], linewidth=2, label=pipeline)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, fontsize=12, fontweight='bold', color='navy')\n",
    "\n",
    "ax.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "ax.set_yticklabels([])\n",
    "\n",
    "plt.title(\"Pipeline Performance Comparison\", size=18, color='darkblue', fontweight='bold', pad=20)\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.1, 1.05), fontsize=10, frameon=False)\n",
    "\n",
    "ax.spines['polar'].set_visible(False)\n",
    "ax.set_facecolor('#f9f9f9')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEATMAP\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data for the pipelines\n",
    "pipelines = [\n",
    "    \"Basic Query Engine\",\n",
    "    \"Sentence Window Query Engine - W/o Reranking\",\n",
    "    \"Sentence Window Query Engine - w/ Re-ranking\",\n",
    "    \"Auto-merging Query Engine - w/ Re-ranking\",\n",
    "    \"Auto-merging Query Engine - w/ Re-ranking and Prompt Engineering\"\n",
    "]\n",
    "\n",
    "# Metrics\n",
    "metrics = ['Answer Correctness', 'Context Relevance', 'Groundedness', 'Answer Relevance']\n",
    "values = np.array([\n",
    "    [0.78, 0.54, 0.51, 0.89],\n",
    "    [0.83, 0.46, 0.28, 0.83],\n",
    "    [0.43, 0.31, 0.55, 0.66],\n",
    "    [0.98, 0.56, 0.48, 0.81],\n",
    "    [0.98, 0.57, 0.40, 0.82]\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(values, annot=True, cmap=\"coolwarm\", xticklabels=metrics, yticklabels=pipelines)\n",
    "plt.title(\"Heatmap: Pipeline Performance Across Metrics\", size=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
