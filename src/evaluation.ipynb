{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages from requirements.txt\n",
    "req_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/requirements.txt'\n",
    "with open(req_path, 'r') as file:\n",
    "    requirements = file.read().splitlines()\n",
    "\n",
    "for package in requirements:\n",
    "    !pip install {package}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragpipelines import *\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "import yaml\n",
    "config_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/config.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "!pip install trulens_eval==0.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for evaluation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trulens_eval.feedback import GroundTruthAgreement\n",
    "from trulens_eval import Tru, OpenAI, Feedback, TruLlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = config['openai_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TruLens and OpenAI provider\n",
    "tru = Tru()\n",
    "openai_provider = OpenAI(model_engine=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tru.reset_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation questions and answers\n",
    "eval_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/eval_questions.xlsx'\n",
    "qa_df = pd.read_excel(eval_path, sheet_name=\"Sheet1\")\n",
    "qa_set = [{\"query\": item[\"Question\"], \"response\": item[\"Answer\"]} for index, item in qa_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metrics\n",
    "# answer relevance\n",
    "f_qa_relevance = Feedback(\n",
    "    openai_provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context relevance\n",
    "f_qs_relevance = Feedback(\n",
    "    openai_provider.qs_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
    ").on_input().on(TruLlama.select_source_nodes().node.text).aggregate(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trulens_eval import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groundedness (sometimes called faithfulness)\n",
    "f_groundedness = (\n",
    "    Feedback(openai_provider.groundedness_measure_with_cot_reasons, name = \"Groundedness\")\n",
    "    .on(Select.RecordCalls.retrieve.rets.collect())\n",
    "    .on_output()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth agreement (sometimes called answer correctness)\n",
    "f_groundtruth = Feedback(\n",
    "    GroundTruthAgreement(qa_set).agreement_measure, name = \"Answer Correctness\"\n",
    ").on_input_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [f_qa_relevance, f_qs_relevance, f_groundedness, f_groundtruth]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create TruLens recorder\n",
    "def get_trulens_recorder(query_engine, app_id):\n",
    "    return TruLlama(query_engine, feedbacks=metrics, app_id=app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base class for RAG pipelines evaluation\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, pipeline_class, app_id, api_key, document_path, llm_model, embed_model):\n",
    "        self.pipeline = pipeline_class(api_key, document_path, llm_model, embed_model)\n",
    "        self.query_engine = self.pipeline.query_engine\n",
    "        self.recorder = get_trulens_recorder(self.query_engine, app_id=app_id)\n",
    "\n",
    "    def evaluate(self, qa_set):\n",
    "        with self.recorder as recording:\n",
    "            for q in qa_set:\n",
    "                self.query_engine.query(q[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = config['openai_api_key']\n",
    "document_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/privacy-pmi.pdf'\n",
    "llm_model = config['llm_model']\n",
    "embed_model = config['embed_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function evaluates a given RAG pipeline by using the TruLens framework\n",
    "def evaluate_pipeline(pipeline, qa_set, app_id, tru, metrics):\n",
    "    # Initialize TruLens recorder for the pipeline\n",
    "    recorder = TruLlama(\n",
    "        app=pipeline.query_engine,  # Assuming the query_engine is the app here\n",
    "        feedbacks=metrics,\n",
    "        app_id=app_id\n",
    "    )\n",
    "\n",
    "    with recorder as recording:\n",
    "        for q in qa_set:\n",
    "            pipeline.query(q[\"query\"])\n",
    "\n",
    "    # Retrieve and return the records and feedback\n",
    "    records, feedback = tru.get_records_and_feedback(app_ids=[app_id])\n",
    "    return records, feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and evaluate each pipeline\n",
    "\n",
    "basic_pipeline = BasicRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                  llm_model=config['llm_model'], embed_model=config['embed_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(basic_pipeline, qa_set, \"0. Basic Query Engine\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_rag_without_rerank = SentenceWindowRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                                               llm_model=config['llm_model'], embed_model=config['embed_model'],\n",
    "                                                               rerank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(sentence_window_rag_without_rerank, qa_set, \"1. Sentence Window Query Engine - W/o Reranking\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_window_rag_with_rerank = SentenceWindowRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                                            llm_model=config['llm_model'], embed_model=config['embed_model'],\n",
    "                                                            rerank=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(sentence_window_rag_with_rerank, qa_set, \"2. Sentence Window Query Engine - w/ re-ranking\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automerging_pipeline = AutoMergingRAGPipeline(api_key=config['openai_api_key'], document_path=document_path,\n",
    "                                              llm_model=config['llm_model'], embed_model=config['embed_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(automerging_pipeline, qa_set, \"3. Automerging Query Engine - w/ re-ranking\", tru, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automerging_pipeline_with_prompting = AutoMergingRAGPipelineWithPrompting(\n",
    "    api_key=config['openai_api_key'],\n",
    "    document_path=document_path,\n",
    "    llm_model=config['llm_model'],\n",
    "    embed_model=config['embed_model']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_pipeline(\n",
    "    automerging_pipeline_with_prompting,\n",
    "    qa_set,\n",
    "    \"4. Auto-Merging Query Engine - w/ re-ranking and prompt engineering\",\n",
    "    tru,\n",
    "    metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launches on http://localhost:8501/\n",
    "tru.run_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tru.stop_dashboard()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
