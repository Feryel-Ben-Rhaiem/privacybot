{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive to access files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to necessary files: requirements.txt, config.yaml, and the document (PDF) to be processed\n",
    "req_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/requirements.txt'\n",
    "\n",
    "config_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/config.yaml'\n",
    "\n",
    "doc_path = '/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/privacy-pmi.pdf'\n",
    "\n",
    "# Load and read the requirements.txt file and config.yaml file\n",
    "with open(req_path, 'r') as file:\n",
    "   requirements = file.read().splitlines()\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Install the required Python packages listed in requirements.txt\n",
    "for package in requirements:\n",
    "    !pip install {package}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "sys.path.append('/content/drive/MyDrive/3. MscIS 22-24/Thesis/final code files/src')\n",
    "from ragpipelines import *\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract necessary configurations such as API key, document path, and model names from the YAML file\n",
    "api_key=config['openai_api_key']\n",
    "document_path=doc_path\n",
    "llm_model=config['llm_model']\n",
    "embed_model=config['embed_model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipelines\n",
    "basic_rag_pipeline = BasicRAGPipeline(\n",
    "    api_key,\n",
    "    document_path,\n",
    "    llm_model,\n",
    "    embed_model)\n",
    "\n",
    "sentence_window_pipeline = SentenceWindowRAGPipeline(\n",
    "    api_key,\n",
    "    document_path,\n",
    "    llm_model,\n",
    "    embed_model)\n",
    "\n",
    "auto_merging_pipeline = AutoMergingRAGPipeline(\n",
    "    api_key,\n",
    "    document_path,\n",
    "    llm_model,\n",
    "    embed_model)\n",
    "\n",
    "auto_merging_pipeline_with_prompting = AutoMergingRAGPipelineWithPrompting(\n",
    "    api_key,\n",
    "    document_path,\n",
    "    llm_model,\n",
    "    embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to query different RAG pipelines based on the user's selection\n",
    "def query_pipeline(query, history, selected_pipeline):\n",
    "    if selected_pipeline == \"Basic RAG\":\n",
    "        response = basic_rag_pipeline.query(query)\n",
    "    elif selected_pipeline == \"Sentence Window (Rerank)\":\n",
    "        sentence_window_pipeline.rerank_enabled = True\n",
    "        response = sentence_window_pipeline.query(query)\n",
    "    elif selected_pipeline == \"Sentence Window (No Rerank)\":\n",
    "        sentence_window_pipeline.rerank_enabled = False\n",
    "        response = sentence_window_pipeline.query(query)\n",
    "    elif selected_pipeline == \"Auto Merging\":\n",
    "        response = auto_merging_pipeline.query(query)\n",
    "    elif selected_pipeline == \"Auto Merging with Prompting\":\n",
    "        response = auto_merging_pipeline_with_prompting.query(query)\n",
    "    history = history or []\n",
    "    history.append((query, response))\n",
    "    return history, history, \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface 1: 5 pipelines displayed at once for comparison\n",
    "def create_demo():\n",
    "    # Initialize history storage for each pipeline\n",
    "    histories = {\n",
    "        \"Basic RAG\": [],\n",
    "        \"Sentence Window (Rerank)\": [],\n",
    "        \"Sentence Window (No Rerank)\": [],\n",
    "        \"Auto Merging\": [],\n",
    "        \"Auto Merging with Prompting\": []  # Add this line\n",
    "    }\n",
    "\n",
    "    def query_pipeline_interface(query, pipeline_selector):\n",
    "        # Retrieve the correct history for the selected pipeline\n",
    "        history = histories[pipeline_selector]\n",
    "\n",
    "        # Use the provided query_pipeline function to get the response\n",
    "        history, _, _ = query_pipeline(query, history, pipeline_selector)\n",
    "\n",
    "        # Ensure the response is a string before appending it to the history\n",
    "        history[-1] = (query, str(history[-1][1]))\n",
    "\n",
    "        # Store the updated history back into the histories dictionary\n",
    "        histories[pipeline_selector] = history\n",
    "\n",
    "        # Return the updated history for the selected pipeline\n",
    "        return history, \"\"\n",
    "\n",
    "\n",
    "    with gr.Blocks(title=\"PrivacyBot - Display 1\", theme=\"Soft\") as demo:\n",
    "        gr.Markdown(\"<h1 style='text-align: center;'>ðŸ”’ðŸ¤– PrivacyBot ðŸ¤–ðŸ”’</h1>\")\n",
    "        gr.Markdown(\"<p style='text-align: center;'>Go ahead and ask me any questions you have about how PMI treats your personal data.</p>\")\n",
    "\n",
    "        # Pipeline selector at the top\n",
    "        with gr.Row():\n",
    "            pipeline_selector = gr.Dropdown(\n",
    "                choices=list(histories.keys()),\n",
    "                value=\"Basic RAG\",\n",
    "                label=\"Select RAG Pipeline\"\n",
    "            )\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                chat_history = gr.Chatbot(elem_id='chatbot', height=500)\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=0.8):\n",
    "                text_input = gr.Textbox(\n",
    "                    show_label=False,\n",
    "                    placeholder=\"Ask anything about how PMI treats your personal data.\",\n",
    "                    container=False\n",
    "                )\n",
    "            with gr.Column(scale=0.2):\n",
    "                submit_button = gr.Button('Send')\n",
    "\n",
    "        # Update the chat history and process the input\n",
    "        submit_button.click(query_pipeline_interface, inputs=[text_input, pipeline_selector], outputs=[chat_history, text_input])\n",
    "        text_input.submit(query_pipeline_interface, inputs=[text_input, pipeline_selector], outputs=[chat_history, text_input])\n",
    "\n",
    "        # Update chat history when a new pipeline is selected\n",
    "        pipeline_selector.change(lambda pipeline: histories[pipeline], inputs=[pipeline_selector], outputs=[chat_history])\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio interface 2: you select 1 pipeline at once, history is saved when you move from 1 pipeline to the other\n",
    "def create_demo():\n",
    "    with gr.Blocks(title=\"PrivacyBot - Display 2\", theme=\"Soft\") as demo:\n",
    "        gr.Markdown(\"<h1 style='text-align: center;'>ðŸ”’ðŸ¤– PrivacyBot ðŸ¤–ðŸ”’</h1>\")\n",
    "        gr.Markdown(\"<p style='text-align: center;'>Go ahead and ask me any questions you have about how PMI treats your personal data.</p>\")\n",
    "\n",
    "        # First row with three columns\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                basic_rag_history = gr.Chatbot(elem_id='chatbot_basic_rag', label=\"Basic RAG Response\", height=600)\n",
    "                basic_rag_input = gr.Textbox(show_label=False, placeholder=\"Ask anything about PMI's privacy policy.\", container=False)\n",
    "                basic_rag_button = gr.Button('Submit')\n",
    "\n",
    "            with gr.Column():\n",
    "                sentence_window_rerank_history = gr.Chatbot(elem_id='chatbot_sentence_window_rerank', label=\"Sentence Window (Rerank) Response\", height=600)\n",
    "                sentence_window_rerank_input = gr.Textbox(show_label=False, placeholder=\"Ask anything about PMI's privacy policy.\", container=False)\n",
    "                sentence_window_rerank_button = gr.Button('Submit')\n",
    "\n",
    "            with gr.Column():\n",
    "                sentence_window_no_rerank_history = gr.Chatbot(elem_id='chatbot_sentence_window_no_rerank', label=\"Sentence Window (No Rerank) Response\", height=600)\n",
    "                sentence_window_no_rerank_input = gr.Textbox(show_label=False, placeholder=\"Ask anything about PMI's privacy policy.\", container=False)\n",
    "                sentence_window_no_rerank_button = gr.Button('Submit')\n",
    "\n",
    "        # Second row with two columns\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                auto_merging_history = gr.Chatbot(elem_id='chatbot_auto_merging', label=\"Auto Merging Response\", height=600)\n",
    "                auto_merging_input = gr.Textbox(show_label=False, placeholder=\"Ask anything about PMI's privacy policy.\", container=False)\n",
    "                auto_merging_button = gr.Button('Submit')\n",
    "\n",
    "            with gr.Column():\n",
    "                auto_merging_prompting_history = gr.Chatbot(elem_id='chatbot_auto_merging_prompting', label=\"Auto Merging with Prompting Response\", height=600)\n",
    "                auto_merging_prompting_input = gr.Textbox(show_label=False, placeholder=\"Ask anything about PMI's privacy policy.\", container=False)\n",
    "                auto_merging_prompting_button = gr.Button('Submit')\n",
    "\n",
    "        # Function to handle submission\n",
    "        def submit_handler(input_text, history, state_name):\n",
    "            if input_text.strip():  # Ensure input is not empty\n",
    "                return query_pipeline(input_text, history, state_name)\n",
    "            else:\n",
    "                return history, history, \"\"\n",
    "\n",
    "        # Set up the pipeline-specific query and output connections\n",
    "        basic_rag_button.click(submit_handler, inputs=[basic_rag_input, basic_rag_history, gr.State(\"Basic RAG\")], outputs=[basic_rag_history, basic_rag_history, basic_rag_input])\n",
    "        sentence_window_rerank_button.click(submit_handler, inputs=[sentence_window_rerank_input, sentence_window_rerank_history, gr.State(\"Sentence Window (Rerank)\")], outputs=[sentence_window_rerank_history, sentence_window_rerank_history, sentence_window_rerank_input])\n",
    "        sentence_window_no_rerank_button.click(submit_handler, inputs=[sentence_window_no_rerank_input, sentence_window_no_rerank_history, gr.State(\"Sentence Window (No Rerank)\")], outputs=[sentence_window_no_rerank_history, sentence_window_no_rerank_history, sentence_window_no_rerank_input])\n",
    "        auto_merging_button.click(submit_handler, inputs=[auto_merging_input, auto_merging_history, gr.State(\"Auto Merging\")], outputs=[auto_merging_history, auto_merging_history, auto_merging_input])\n",
    "        auto_merging_prompting_button.click(submit_handler, inputs=[auto_merging_prompting_input, auto_merging_prompting_history, gr.State(\"Auto Merging with Prompting\")], outputs=[auto_merging_prompting_history, auto_merging_prompting_history, auto_merging_prompting_input])\n",
    "\n",
    "        # Connect the Textbox submission (Enter key) to the Button click event\n",
    "        basic_rag_input.submit(submit_handler, inputs=[basic_rag_input, basic_rag_history, gr.State(\"Basic RAG\")], outputs=[basic_rag_history, basic_rag_history, basic_rag_input])\n",
    "        sentence_window_rerank_input.submit(submit_handler, inputs=[sentence_window_rerank_input, sentence_window_rerank_history, gr.State(\"Sentence Window (Rerank)\")], outputs=[sentence_window_rerank_history, sentence_window_rerank_history, sentence_window_rerank_input])\n",
    "        sentence_window_no_rerank_input.submit(submit_handler, inputs=[sentence_window_no_rerank_input, sentence_window_no_rerank_history, gr.State(\"Sentence Window (No Rerank)\")], outputs=[sentence_window_no_rerank_history, sentence_window_no_rerank_history, sentence_window_no_rerank_input])\n",
    "        auto_merging_input.submit(submit_handler, inputs=[auto_merging_input, auto_merging_history, gr.State(\"Auto Merging\")], outputs=[auto_merging_history, auto_merging_history, auto_merging_input])\n",
    "        auto_merging_prompting_input.submit(submit_handler, inputs=[auto_merging_prompting_input, auto_merging_prompting_history, gr.State(\"Auto Merging with Prompting\")], outputs=[auto_merging_prompting_history, auto_merging_prompting_history, auto_merging_prompting_input])\n",
    "\n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_interface():\n",
    "    demo = create_demo()\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_interface()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
